{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdOOmbWKlvWV"
   },
   "source": [
    "# ME 536 TERM PROJECT\n",
    "## FELINE DETECTOR\n",
    "### SUBMITTED BY : REHA OĞUZ USLU\n",
    "### 2308500\n",
    "\n",
    "#### Additioal Drive Link For Dataset and Reports : https://drive.google.com/drive/folders/1ArC6LMZw1vEjuvnoQ9DavJD4cDkjdi-U?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vFeqJiQqlklE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 17:13:17.671343: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***PREPROCESSING***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnPHs_tllq8o"
   },
   "source": [
    "### Preprocessing Training Data\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **There exist 4 clsses such as lion, tiger, leopard and cheetah.**\n",
    " * **All classes have 700 training image and 300 test image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "0K00FR7flqRe",
    "outputId": "b79eb302-2da1-4625-c493-3be9044774c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2800 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    'training_dataset',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 1,\n",
    "    class_mode = 'categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crfpuYYNtxEx"
   },
   "source": [
    "### Preprocessing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = train_datagen.flow_from_directory(\n",
    "    'test_dataset',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 1,\n",
    "    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***CNN***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EsWPTcTuhco"
   },
   "source": [
    "### Installing CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Firstly convolution and maxpooling is done by convolutional neural network.**\n",
    "* **Keras is used for CNN from TensorFlow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0g4_eR3Uu7AX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 17:14:03.705093: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5x3KL_RvPyC"
   },
   "source": [
    "### First Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wfLTlg-0vSLt"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=16,kernel_size=3,activation = 'relu',input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K_dtgsUwgeH"
   },
   "source": [
    "### First Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qyuAeEghwhbt"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nIFQBvgxdro"
   },
   "source": [
    "### Second Layer For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "miknobMVxfX-"
   },
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=8,kernel_size=3,activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now we will try to use specified kernel/filter on our data to improve our results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800/2800 [==============================] - 9s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "data = cnn.predict(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Check dimensions of data**\n",
    "    * **Expect to see 4D tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 14, 14, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Okey!! 1. dimension indicates number of data (2800 images)**\n",
    "* **2. and 3. dimensiones  indicate convolved and pooled images height and width (rememeber initially we have 64x64 images).**\n",
    "* **Last dimension shows the batch size/channels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***FILTERING***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Filter/Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = tf.constant([\n",
    "    [0, -1,  0],\n",
    "    [-1, 4, -1],\n",
    "    [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0],[0, -1,  0],[-1, 4, -1], [0, -1,  0],\n",
    "    [0, -1,  0],[-1, 4, -1], [0, -1,  0]\n",
    "    \n",
    "], dtype=tf.float32, name='kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = tf.reshape(kernel, [3, 3, 1, 32], name='kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Data With Specified Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_data= tf.nn.conv2d(\n",
    "    data, kernel, [1,1,1,1], 'VALID', \n",
    "    data_format='NHWC', dilations=[1, 1, 1, 1], name='kerneled'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Expect to see 12x12 image after convolution (remember we have images 14x14)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2800, 12, 12, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use 5x5 Kernel For This Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel2 = tf.constant([\n",
    "    [1, 1, 1, 1,1],\n",
    "    [1,-1,-1,-1,1],\n",
    "    [1,-1,-1,-1,1],\n",
    "    [1,-1,-1,-1,1],\n",
    "    [1, 1, 1, 1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "    [1,1,1,1,1],[1,-1,-1,-1,1],[1, -1, -1,-1,1],[1,-1,-1,-1,1],[1,1,1,1,1],\n",
    "\n",
    "\n",
    "], dtype=tf.float32, name='kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel2 = tf.reshape(kernel2, [5, 5, 1, 32], name='kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data2= tf.nn.conv2d(\n",
    "    filtered_data, kernel2, [1,1,1,1], 'VALID', \n",
    "    data_format='NHWC', dilations=[1, 1, 1, 1], name='kerneled'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2800, 8, 8, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now we need to flatten our data for PCA Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatter = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatter.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "flatten_data = flatter.predict(filtered_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **We expect to see 2800 data point with 8x8x32 = 2048 dimensions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 2048)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***PCA ANALYSIS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Mean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZeroMean(M):\n",
    "    return M - M.mean(axis=1).reshape((M.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zm = ZeroMean(flatten_data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,VT = np.linalg.svd(data_zm,full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Let look sigma values graphically to estimate real rank.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb910d1dd00>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(S,'r*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[681.514   568.7508  484.87863 433.18994 368.97595 362.8315  321.13245\n",
      " 307.0515  282.2572  262.87512 260.6739  253.4559  243.00989 234.03564\n",
      " 230.56721]\n"
     ]
    }
   ],
   "source": [
    "print(S[0:15])l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **It looks like real rank is 1.**\n",
    "* **Check rank by np.linalg.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(data_zm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **1024?? WOW**\n",
    "* **Check energy method!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for i in range(len(S)-1):\n",
    "    t = t + (S[i])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0 \n",
    "r = 0 \n",
    "for i in range(len(S)):\n",
    "    k = k + (S[i])**2\n",
    "    if k >= 0.9*t :\n",
    "        break\n",
    "    r = r + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total energy:3987717.9350517844\n",
      "Rank Energy:3589100.8272624\n",
      "Real Rank:126\n"
     ]
    }
   ],
   "source": [
    "print(f'Total energy:{t}\\nRank Energy:{k}\\nReal Rank:{r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Energy method indicates rank 8, however it is obvious that our rank is actualy 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_mat = 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_pca = np.matmul(U[:,rank_mat].reshape(-1,1),VT[rank_mat,:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Again take transpose, since we take transpose before zero mean data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_pca = data_w_pca.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2800, 2048)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_w_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=8, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **We need result matrix for ANN to learn which data point belongs to which cluster.**\n",
    "* **Also y_set array should be one hot encoded to be compatible with ANN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(700)\n",
    "b = 1*np.ones(700)\n",
    "c = 2*np.ones(700)\n",
    "d = 3*np.ones(700)\n",
    "\n",
    "y_set = np.concatenate((a,b,c,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_set_ =tf.one_hot(\n",
    "    y_set,\n",
    "    4,\n",
    "    on_value=1,\n",
    "    off_value=0,\n",
    "    axis=-1,\n",
    "    dtype=None,\n",
    "    name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "88/88 [==============================] - 1s 2ms/step - loss: 1.3866 - accuracy: 0.2482\n",
      "Epoch 2/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3861 - accuracy: 0.2579\n",
      "Epoch 3/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3860 - accuracy: 0.2511\n",
      "Epoch 4/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3857 - accuracy: 0.2664\n",
      "Epoch 5/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3854 - accuracy: 0.2671\n",
      "Epoch 6/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3853 - accuracy: 0.2593\n",
      "Epoch 7/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3855 - accuracy: 0.2621\n",
      "Epoch 8/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3853 - accuracy: 0.2671\n",
      "Epoch 9/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3857 - accuracy: 0.2636\n",
      "Epoch 10/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3853 - accuracy: 0.2671\n",
      "Epoch 11/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3851 - accuracy: 0.2693\n",
      "Epoch 12/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3854 - accuracy: 0.2636\n",
      "Epoch 13/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3853 - accuracy: 0.2661\n",
      "Epoch 14/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3853 - accuracy: 0.2629\n",
      "Epoch 15/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3856 - accuracy: 0.2664\n",
      "Epoch 16/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3850 - accuracy: 0.2671\n",
      "Epoch 17/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3852 - accuracy: 0.2664\n",
      "Epoch 18/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3851 - accuracy: 0.2664\n",
      "Epoch 19/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3851 - accuracy: 0.2689\n",
      "Epoch 20/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3853 - accuracy: 0.2621\n",
      "Epoch 21/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3852 - accuracy: 0.2668\n",
      "Epoch 22/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3850 - accuracy: 0.2679\n",
      "Epoch 23/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3850 - accuracy: 0.2639\n",
      "Epoch 24/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3851 - accuracy: 0.2664\n",
      "Epoch 25/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3850 - accuracy: 0.2679\n",
      "Epoch 26/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3851 - accuracy: 0.2714\n",
      "Epoch 27/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3848 - accuracy: 0.2682\n",
      "Epoch 28/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3854 - accuracy: 0.2664\n",
      "Epoch 29/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3851 - accuracy: 0.2654\n",
      "Epoch 30/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3849 - accuracy: 0.2693\n",
      "Epoch 31/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3849 - accuracy: 0.2643\n",
      "Epoch 32/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3848 - accuracy: 0.2679\n",
      "Epoch 33/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3848 - accuracy: 0.2632\n",
      "Epoch 34/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3847 - accuracy: 0.2679\n",
      "Epoch 35/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3849 - accuracy: 0.2686\n",
      "Epoch 36/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3846 - accuracy: 0.2657\n",
      "Epoch 37/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3847 - accuracy: 0.2639\n",
      "Epoch 38/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3849 - accuracy: 0.2621\n",
      "Epoch 39/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3847 - accuracy: 0.2671\n",
      "Epoch 40/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3847 - accuracy: 0.2679\n",
      "Epoch 41/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3847 - accuracy: 0.2646\n",
      "Epoch 42/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3845 - accuracy: 0.2668\n",
      "Epoch 43/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3848 - accuracy: 0.2661\n",
      "Epoch 44/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3846 - accuracy: 0.2679\n",
      "Epoch 45/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3844 - accuracy: 0.2671\n",
      "Epoch 46/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3842 - accuracy: 0.2668\n",
      "Epoch 47/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3844 - accuracy: 0.2625\n",
      "Epoch 48/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3845 - accuracy: 0.2700\n",
      "Epoch 49/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3845 - accuracy: 0.2686\n",
      "Epoch 50/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3844 - accuracy: 0.2671\n",
      "Epoch 51/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3846 - accuracy: 0.2675\n",
      "Epoch 52/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3841 - accuracy: 0.2682\n",
      "Epoch 53/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3841 - accuracy: 0.2668\n",
      "Epoch 54/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3842 - accuracy: 0.2675\n",
      "Epoch 55/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3841 - accuracy: 0.2679\n",
      "Epoch 56/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3843 - accuracy: 0.2643\n",
      "Epoch 57/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3841 - accuracy: 0.2679\n",
      "Epoch 58/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3838 - accuracy: 0.2707\n",
      "Epoch 59/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3841 - accuracy: 0.2639\n",
      "Epoch 60/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3840 - accuracy: 0.2668\n",
      "Epoch 61/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3846 - accuracy: 0.2639\n",
      "Epoch 62/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3840 - accuracy: 0.2679\n",
      "Epoch 63/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3838 - accuracy: 0.2646\n",
      "Epoch 64/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3839 - accuracy: 0.2675\n",
      "Epoch 65/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3836 - accuracy: 0.2675\n",
      "Epoch 66/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3839 - accuracy: 0.2689\n",
      "Epoch 67/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3838 - accuracy: 0.2700\n",
      "Epoch 68/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3837 - accuracy: 0.2643\n",
      "Epoch 69/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3842 - accuracy: 0.2682\n",
      "Epoch 70/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3839 - accuracy: 0.2664\n",
      "Epoch 71/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3841 - accuracy: 0.2632\n",
      "Epoch 72/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3837 - accuracy: 0.2650\n",
      "Epoch 73/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3838 - accuracy: 0.2679\n",
      "Epoch 74/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3836 - accuracy: 0.2661\n",
      "Epoch 75/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3837 - accuracy: 0.2689\n",
      "Epoch 76/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3836 - accuracy: 0.2661\n",
      "Epoch 77/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3838 - accuracy: 0.2636\n",
      "Epoch 78/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3839 - accuracy: 0.2682\n",
      "Epoch 79/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3833 - accuracy: 0.2679\n",
      "Epoch 80/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3840 - accuracy: 0.2625\n",
      "Epoch 81/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3835 - accuracy: 0.2718\n",
      "Epoch 82/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3834 - accuracy: 0.2718\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3836 - accuracy: 0.2664\n",
      "Epoch 84/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3834 - accuracy: 0.2668\n",
      "Epoch 85/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3836 - accuracy: 0.2675\n",
      "Epoch 86/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3833 - accuracy: 0.2657\n",
      "Epoch 87/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3832 - accuracy: 0.2643\n",
      "Epoch 88/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3833 - accuracy: 0.2689\n",
      "Epoch 89/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3832 - accuracy: 0.2639\n",
      "Epoch 90/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3831 - accuracy: 0.2704\n",
      "Epoch 91/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3831 - accuracy: 0.2725\n",
      "Epoch 92/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3832 - accuracy: 0.2668\n",
      "Epoch 93/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3833 - accuracy: 0.2650\n",
      "Epoch 94/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3833 - accuracy: 0.2682\n",
      "Epoch 95/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3831 - accuracy: 0.2721\n",
      "Epoch 96/100\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 1.3833 - accuracy: 0.2700\n",
      "Epoch 97/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3830 - accuracy: 0.2768\n",
      "Epoch 98/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3828 - accuracy: 0.2682\n",
      "Epoch 99/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3829 - accuracy: 0.2739\n",
      "Epoch 100/100\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 1.3831 - accuracy: 0.2768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb8fdaeb790>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(data_w_pca, y_set_, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing  Single Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image = train_datagen.flow_from_directory(\n",
    "    'test_image',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 1,\n",
    "    class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "test_image = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 14, 8)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_img = tf.nn.conv2d(\n",
    "    test_image, kernel, [1,1,1,1], 'VALID', \n",
    "    data_format='NHWC', dilations=[1, 1, 1, 1], name='kerneled'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_img_fnl = tf.nn.conv2d(\n",
    "    filtered_test_img, kernel2, [1,1,1,1], 'VALID', \n",
    "    data_format='NHWC', dilations=[1, 1, 1, 1], name='kerneled'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "filtered_test_img = flatter.predict(filtered_test_img_fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_img = ZeroMean(filtered_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2048)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_test_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 8)         1160      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,608\n",
      "Trainable params: 1,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "result = ann.predict(filtered_test_img)\n",
    "training_set.class_indices\n",
    "      \n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tresholds For Nolvalty Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiger Similarity : 23.25342893600464% \n",
      "Lion Similarity : 21.371516585350037% \n",
      "Leopard Similarity : 18.614377081394196% \n",
      "Cheetah Similarity : 36.76068186759949% \n"
     ]
    }
   ],
   "source": [
    "print(f\"Tiger Similarity : {result[0][2]*100}% \\nLion Similarity : {result[0][1]*100}% \\nLeopard Similarity : {result[0][0]*100}% \\nCheetah Similarity : {result[0][3]*100}% \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Printer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW \n",
      "NEW KIND OF FELINE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = max(result[0])\n",
    "if m*100<50 and m*100>30:\n",
    "    print('NEW \\nNEW KIND OF FELINE \\n')\n",
    "elif m*100<=30:\n",
    "    print('NEW \\nNOT FELINE\\n')\n",
    "else:\n",
    "    print(f'NOT NEW')\n",
    "    \n",
    "    if m == result[0][0]:\n",
    "        print('IT IS TIGER!')\n",
    "    elif m == result[0][1]:\n",
    "        print('IT IS LION!')\n",
    "    elif m == result[0][0]:\n",
    "        print('IT IS LEOPARD!')\n",
    "    else:\n",
    "        print('IT IS CHETAH!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
